{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpamClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqO-1PNqpU5p",
        "colab_type": "code",
        "outputId": "5790c169-2645-4036-a9aa-100ea0545998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "import nltk\n",
        "from collections import defaultdict\n",
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "import json\n",
        "#create a dictionary of spam and ham that keeps the count of each words in the spam and ham traing dataset\n",
        "#creates a set vocabulary that contains the unique words in the training set\n",
        "def createDict():\n",
        "  spam_train = os.listdir('drive/My Drive/Colab Notebooks/Dataset/train/spam/')\n",
        "  spam_test = os.listdir('/content/drive/My Drive/Colab Notebooks/Dataset/test/spam/')\n",
        "  ham_train = os.listdir('/content/drive/My Drive/Colab Notebooks/Dataset/train/ham/')\n",
        "  ham_test = os.listdir('/content/drive/My Drive/Colab Notebooks/Dataset/test/ham/')\n",
        "  vocabulary= set()\n",
        "  # special_characters = \"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
        "  spam = defaultdict(int)\n",
        "  ham = defaultdict(int)\n",
        "  count_spam = 0\n",
        "  count_ham = 0\n",
        "  result = []\n",
        "  count_spamfiles = 0\n",
        "  count_hamfiles = 0\n",
        "  for file in spam_train:\n",
        "      count_spamfiles += 1\n",
        "      f = open('/content/drive/My Drive/Colab Notebooks/Dataset/train/spam/'+file, errors='ignore')\n",
        "      nopunc = \"\"\n",
        "      line = f.read().replace('\\n', ' ')\n",
        "      for char in line:\n",
        "          if char not in string.punctuation:\n",
        "              nopunc += char\n",
        "\n",
        "      # print(nopunc)\n",
        "      words = nopunc.split()\n",
        "      # clean_words = [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
        "      for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "          spam[word] += 1\n",
        "          vocabulary.add(word)\n",
        "          count_spam += 1\n",
        "\n",
        "  for file in ham_train:\n",
        "      count_hamfiles += 1\n",
        "      f = open('/content/drive/My Drive/Colab Notebooks/Dataset/train/ham/'+file, 'r')\n",
        "      nopunc = \"\"\n",
        "      line = f.read().replace('\\n', ' ')\n",
        "      for char in line:\n",
        "          if char not in string.punctuation:\n",
        "              nopunc += char\n",
        "        \n",
        "      words = nopunc.split()        \n",
        "      for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "          ham[word] += 1\n",
        "          vocabulary.add(word)\n",
        "          count_ham += 1\n",
        "\n",
        "\n",
        "  print(\"voc:\",len(vocabulary))\n",
        "  print(\"count of spam\", count_spam)\n",
        "  print(\"count of ham\",count_ham)\n",
        "  print(count_hamfiles)\n",
        "  print(count_spamfiles)\n",
        "  # return count_spam, count_ham, count_spamfiles, count_hamfiles, len(vocabulary),spam,ham\n",
        "  dict = {\"spamwords\":spam,\n",
        "          \"hamwords\":ham,\n",
        "          \"spam_count\":count_spam,\n",
        "          \"ham_count\":count_ham,\n",
        "          \"no_unique_words\": len(vocabulary)       \n",
        "          }\n",
        "  with open('data1.json', 'w') as outfile:\n",
        "      json.dump(dict, outfile)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "voc: 10274\n",
            "count of spam 14354\n",
            "count of ham 34846\n",
            "340\n",
            "133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlFiDswZUOo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculate the probability of each word in the tarining dataset\n",
        "def calculateProbability(word, word_dict, b_c):\n",
        "    ans = (word_dict[word]+1)/(b_c)\n",
        "    return ans**(1./10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXFwoIEdvYL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test the spam and ham test dataset\n",
        "def testing(count_spam,count_ham, P_spam,P_ham,spam,ham, vocabulary):\n",
        "  result = []\n",
        "  ctspam = 0\n",
        "  ctham = 0\n",
        "  ham_test = os.listdir('/content/drive/My Drive/Colab Notebooks/Dataset/test/ham/')\n",
        "  k = 1\n",
        "  for file in ham_test:\n",
        "    P1 = P_spam\n",
        "    P2 = P_ham\n",
        "    f = open('drive/My Drive/Colab Notebooks/Dataset/test/ham/'+file, errors='ignore')\n",
        "\n",
        "    clean_words = f.read().replace('\\n', ' ').split()\n",
        "    for word in clean_words:\n",
        "      P1 *= calculateProbability(word, spam, vocabulary+count_spam)\n",
        "      P2 *= calculateProbability(word, ham, count_ham+vocabulary)\n",
        "\n",
        "    if P1 >= P2:\n",
        "        result.append('spam')\n",
        "        ctspam += 1\n",
        "    else:\n",
        "        result.append('ham')\n",
        "        ctham += 1\n",
        "    print(\" \\r Processing\", k ,\"file out of\", len(ham_test), end = \" \")\n",
        "    k+=1\n",
        "  spam_test = os.listdir('/content/drive/My Drive/Colab Notebooks/Dataset/test/spam/')\n",
        "  k = 1\n",
        "  result = []\n",
        "  for file in spam_test:\n",
        "    P1 = P_spam\n",
        "    P2 = P_ham\n",
        "    f = open('drive/My Drive/Colab Notebooks/Dataset/test/spam/'+file, errors='ignore')\n",
        "\n",
        "    clean_words = f.read().replace('\\n', ' ').split()\n",
        "    for word in clean_words:\n",
        "      P1 *= calculateProbability(word, spam, vocabulary+count_spam)\n",
        "      P2 *= calculateProbability(word, ham, count_ham+vocabulary)\n",
        "\n",
        "    if P1 >= P2:\n",
        "        result.append('spam')\n",
        "        ctspam1 += 1\n",
        "    else:\n",
        "        result.append('ham')\n",
        "        ctham1 += 1\n",
        "    print(\" \\r Processing\", k ,\"file out of\", len(ham_test), end = \" \")\n",
        "    k+=1\n",
        "  return ctspam, ctham, ctspam1, ctham1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SanUIGMgaCyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate accuracy\n",
        "def calculateAccuracy(ctspam, ctham, ctspam1, ctham1):\n",
        "  correct_classification = ctspam1 + ctham\n",
        "  accuracy = correct_classification/(ctspam + ctham + ctspam1 + ctham1)\n",
        "  return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5s5ZUquUuZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_spam, count_ham, count_spamfiles, count_hamfile, no_unique_words,spam,ham = createdict()\n",
        "P_spam = count_spamfiles/(count_spamfiles + count_hamfile)\n",
        "P_ham = count_hamfile/(count_spamfiles + count_hamfile)\n",
        "print(P_spam)\n",
        "print(P_ham)\n",
        "ctspam, ctham, ctspam1, ctham1 = testing(count_spam, count_ham, P_spam, P_ham, spam, ham, no_unique_words)\n",
        "accuracy = calculateAccuracy(ctspam, ctham, ctspam1, ctham1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vym5TlDqZOHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS3JWRUXa3jA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}